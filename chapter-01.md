## Chapter 1. Searching for the best course of action

Consider we're trying to automate a warehouse. We have a representation of the exact location of every item in the warehouse.
To retrieve a specific item, one may need to move first other items that are in the item's way (or above it for example).
Now, we're given a request for an item.
If the item is accessable, a robotic arm can reach for it, place it on the moving belt, and we'll collect it on the other end of the belt. If however the item is somewhat hidden behind other items, some __planning__ is needed.
For example moving the blocking items to another place first, and so revealing the desired item. Once the item becomes accessible, the robotic arm can proceeed as before.
Planning can be done by __searching__ the space of options. Options are actions such as picking an item and placing it in another place.
If the current warehouse representation is a state, then the warehouse as it will look after a potential action is another state.
Planning should come up with an efficient and effective series of actions that will lead in theory to the item being accessible.
Once the plan is computed by a search algorithm, it is then executed, moving items to reveal the desired item. If all goes well, the final locations of the items, reflected by the software representation, enables the robotic arm to fetch the desire item, and sending it to us using the moving belt.

To better understand the concept of search, let's think about the chess game. We want to build a software that plays agaist a human and give a good fight. For the sake of discussion, we're in the initial board state and the software plays the White. The software can make any legitimate move, a thing that will hopefully bring it closer to victory. Once a move is done by the White, given that we haven't won yet, or the board is not a stalemate then the Black gets a change to make his move. The move of the Black also changes the state of the board of course. So a planning here should take into account also that the other player will also try to win, and hence we should avoid leaving the board in a winnable state (winnable for the other side). In some problems or puzzles it is possible to exlusively explore in reasonable time, all possible developments in the state space and select the best path (ex. potentially win while never lose in Tic-Tac-Toe). In chess and in many other games or real world settings, a search is possible yet only to a certain depth. Therefore will not be able to see all future developments, and we must act based on some __heuristic__. For example we can estimate if a chess board is good for us based on the number of squares that we are in control of minus the number of squares that the opponent is in control of. Such heuristic helps both in directing the search, and in choosing what seems to be as the best move or action.

Search and planning are considered __Artificial Inteligence__ (AI). Under the wide umbrella of AI, we find approaces for solving problems, not by instructing the machine directly __How__ to address a challenge, but rather __What__ is a desired outcome. A lot of the concepts mentioned in this book fall under AI, for example, RL and also supervised and unsupervised learning. What is not AI? If we have a well defined algorithms, smart as they are, but that we can code and proof, then those will not be considered AI. Most traditional software is not AI. If a software challenge can be solved in an effective and cost efficient way without AI, this is usually the prefered option. This will be because with AI we lose somewhat control of what exactly does the software do. How does it decide or act. Is it safe? There is a lot of of research attempting to mitigate the risks of not well enough understood AI systems. __Explainable AI__ (XAI) and __Model Interpretability__ are two pharses to Google/Bing with this respect. A lot of times we will resort to AI, simply because we don't know to solve those problems otherwise. Where required and makes sense, we will make use of risk-proportional sound evaluation so we can achieve state-of-the-art solutions and still have a good sleep.
With chess we wanted to convey to the software that it should find the way to win. We show the machine what it means to win or to lose, and we want it to come up with the sequence of moves that will lead to the final winning move. Not everything is in our control. The opponent chooses its own moves. He/she also wants to win or at least to avoid a loss. More than that, knowning that our software cannot inspect all the possible branches from the current state to all final states (win/lose/statemate), we had to help the search by providing a hand tailured heuristic. In the example above, the heuristic is a function from a board to a real value representing the value of the board or state from the player's viewpoint.

While chess seams to be hard, the game is still well defined and controlled setting. The number of levers that a player can pull at any given turn is finite and small. A player can choose any one of a valid next move. Then he/she/it needs to wait for their next turn. When selecting a move, there is certenty regarding the next board state, as dictated by the game's rules. Both players have full knowledge of the current state, can write down past moves, and can plan and speculate about future moves. The only uncertenty is what the opponent will do next. And still chess is very interesting as we cannot see all the possible futures and who will win.

A common introduction to RL is the __k-armed bandits__. You are standing front of k slot machines in Las-Vegas. To play a machine you pay $1 and pull the relevant lever. You then collect the winnings, if any, and continue if you wish and can afford it. For simplicity let's assume you play exactly 20 rounds. It is known to you that each of those machines has a different expected reward (average win). Would be nice to know which of those has the highest reward. Unfortunately there is no external hint. What you can do, is to try all and attempt to figure-out which one it is. The expected reward of a machine will not change over time, and that is nice to know. However since we're talking about expectations, we're never sure if we got it right. For example, let's say k=3. You've tried the three machines and got the following wins, $2, $0, $0 respectively. It seems that the first machine is the best, yet it might have been by chance. You're now left with a dilemma. To exploit your current belief and stick with the first machine, or explore a little more the other machines, just to be sure. What is the best policy here? Can we search for the optimal solution? With any additional exploration we increase our understanding of the environment, yet also waste a chance to exploit our knowledge. Assuming we are trying to maximizing the total amount of money we have in our pocket at the end of the day, we might decide to invest in exploration also, but probably not when we make the last attempt (the 20'th round in the example).

Let's briefly compare chess, and the k-armed bandits. In both we are trying to find the best course of action. In chess the rules are clear. It is a two playes game with interliving turns. When we make a move the result is deterministic. Then the opponent makes a move which is in his/her control. In chess there is a natural end with an outcome of Win/Lose/Stalemate. In the k-armed bandits, there is an element of uncertaintly, that is what is the expected reward of each of the machines. And then an element of randomness, what will the current pull reward us (even if we knew for certain the expected reward for the specific machine).
In chess there is an advantage to those who can evaluate better a board, plan deeper, remember patterns and learn from experience and from the masters. In the k-armed bandits, there is an advantage to a smart exploitation/exploration scheme. The outcome of the k-armed bandits is, for example, the total wins after a fixed amount of rounds, which we don't know if was the maximum possible or not, but we can compare an average performance over a large number of trials with another strategy.

If we try to build a __self-driving car__. Which of above is closer? Is self-driving car more like playing chess or rather like the k-armed bandits? Who is our opponent in self-driving car? The pedestrian trying to cross the road? The rain? What does it mean to win in the self-driving car setting? The interesting thing that the real world adds over chess is uncertainty. Also there are a lot of good outcomes and at least as many bad endings. Randomness in the environment and in other players actions add to randomness in the responce to our actions. We are less in control of the next board state. It is far beyond what the other player will do in his/her turn. It might also be that our attempt to move to a square with a chess piece, ended in another piece moving accidently. We must continusly monitor the board. The observations that we get, for example from a video stream, must be continuously analysed. Our belief of the current world's state, must be updated while operating in it.
If we go back to the example with the warehouse, planning of actions is paramount. But then we must continuesly check if we see what we expect to see. Is there indeed a red box where we believe it should be? Has the arm of the robot reached the hight we wanted it to in?

### Making the most of every learning opportunity

If we build a self-driving car, and send it to the field before it is ready, it may not end well. So we better first record a human behavior and let the software learn by immitations. A similar thing can be said about chess. We don't want to deploy the program to human players before it is ready. So, we'll first make it play offline against itself or alternative software implementation and learn from that. We have also many recorded games by chess masters. Yet bacause of the many possibilities, we will most likely not face the same situation on the road, or the same board in the chess game, that we've encounter before. So what we need to learn is How to play chess or How to drive. It is hard to explain and create an algorithm for driving or even for playing chess, but RL can help us get there by learning from the examples what behavior leads to good outcomes. When a chess game is lost, the machine should learn to do something different. But it is not clear which of the moves that lead to the loss should be attributed with the loss. With RL we can learn the how, while not being sure about individual actions, from our own experience, and from watching others do the task. For example, consider learning how to play chess from the actions of the opponent. There is another interesting way of solving search like optimizations in not fully understood settings, __Genetic Algorithms__, or __Evolutionary Algorithms__. Those are inspired by biology. Many possible programmed individuals are exposed to a simulated enviroment and to examples. Some survive (because they win the chess for example) and some pass away. Then new generations of possible solutions are born, and so the survivel of the fittest, and "natural selection" achieve the learning task. With RL, the idea is to learn from expirence even more. Every action that we or anybody else takes, should give us a hint of the best policy.

### Being adjustable




weak opponent
