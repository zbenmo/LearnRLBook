## Chapter 3. Bellman Equation

To develop a good or even the best policy, it will help us to formulize what we're trying to optimize. We should account for immediate and short time rewards, but also for future rewards. We shall use a discount factor, &gamma; where appropriate. Starting from a state *s* as an example, we know that we can take action *a1* or action *a2*. Taking action *a1* shall reward us with +1 and shall live us at state *s1* while action *a2* shall reward us with +0 and leave us at *s2*. What is the best policy here? Should with take action *a1* or action *a2*? Above the assumption was of certaintly with respect to the rewards and the next state, for simplicitly. In order to choose between *a1* and *a2* we need to know also something about *s1* and *s2*. Let's use here *&gamma;=0.9*. If *V(s1)=1* and *v(s2)=2* then taking *a1* will reward us in a total of +1 + &gamma;\*V(s1) = +1.9, while taking *a2* will reward us +1.8, hence the better policy would be to take action *a1*.

We used before __value functions__, *V(s)*. Back then it was the optimal value function, *V<sup>\*</sup>(s)*, which is the maximum value we can achieve at each state *s*. A value funtion can be defined with respect to a policy. *V<sub>&pi;</sub>(s)*. 
Given a policy, and following the policy at every step, each state has a potential for total reward. The same state, given a different policy may have a different potential for total reward. How can that be? Consider a chess policy, never to lose the queen ðŸ‘‘. It is a good rule of thumb. But maybe a chess master sees a board state where sacrifacing the queen leads to a victory. That would be a better policy, with higher value for that board state. In a more philosofical interpretation, consider two humans born into the same situation in life, one living a happy life, while the other strugles. It can be attributed to luck. But also to different choises in life taken by the internal policy, instincts, or interpratations of the benefits or risks in each action. What people consider rewarding will influence their actions, but this is beyond diffent value functions for different policies. For diffent policies in this discussion we still assume the environment behaves the same with respect to state transitions and rewards. A policy that allows for exploration may also help in achieving higher value functions. Maybe you've listen to a good advice in your yough, rather than sticked to your safety-zone and hence discovered new possibilities and achieved more in your life.

Back to RL reallity! The optimal value function is the value function for an optimal policy. There is exactly one optimal value function, yet potentially more than one policy can lead to it. If we want to achieve an optimal policy given that we know the optimal value function, we still need to know the dynamics of the environment. Because if we take an action in a state, unless we know the dynamics of the environment, we don't know what is the probability to land in each future state. Let's assume for now that we do know the dynamics of the environment *p(s'|s, a)* and *r(s, a)*. *p(s'|s, a)* is the probability to land at state *s'* given that we start at state *s* and take action *a*. *r(s, a)* is the expected reward from state *s* if action *a* is taken. Maybe the reward is related to the state *s'* that we land in, and then it might be better expressed it as *r(s, a, s')*. We'll use probability and expectation as to be general and account for environments with some uncertainty. For example taking the action *forward* may result in the state one step ahead (say with probability 0.9). But a slight slip can bring us to the left, 0.04, or to the right, 0.06. In settings where the environment is deterministic this can be expressed as one probability equals to 1, while the rest to 0.

__state-value__ functions are also defined with respect to a policy, *q<sub>&pi;</sub>(s, a)*. The state-value function tells us the value one can expect from taking a specific action in a specific state, *and then following the policy!*. And so the policy is *not* consulted in the first move, but is followed in all next moves. If you recall, we considered three approaces. One is to figure out a policy *&pi;* that will tell us what to do at every state. The second option is to learn state-value function *Q* and using that function, given we need to take an action and we are in a specific state, quickly compare the value for all possible actions from that state. The third approach was to learn the expected value of the states *V*. The optimal state-value function *q<sup>\*</sup>(s, a)* is the function that tells us for every state and for every available action from that state, the maximum value that we can expect. The optimal state-value function is associated with any optimal policy. Following any optimal policy is expected to yield the values of the optimal state-value function. If we happen to know the optimal state-value function, then at any state we pick the action with the highest expected state-action value. Note that unlike value functions, here we don't need to know the dynamics of the environment *p(s'|s, a)* and *r(s, a)*. The knowledge about the environment is already compressed into the state-value function, and we just need to follow it.

