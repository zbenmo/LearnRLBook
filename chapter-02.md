## Chapter 2. Ingridiant of a Reinforcement Learning setting

We already mentioned an __agent__ acting in an __environment__. What do we know about the environment? Sometimes we know a lot, as in the chess game. We know the rules. The agent, a software that plays one of the sides, should make the moves that will make it win or at least be a strong contestant. The unknown are the moves of the other player, for example a human. We can guess what he/she will do, but there is no certainty about it. A __state__ in the chess game is natively the board state, when the turn is ours (of the agent). From that state (for example the starting position when we are playing the white), the agent will need to make a sequence of decisions regarding its moves, or __actions__. While the agent can plan in advance a few moves ahead, only the current move is taken and communicated to the environment (which in turn will inform the human player). When the next turn comes, we learn from the environment about the move that the human took. The original plan can be followed if still relevant, or another plan needs to be decided on. Given a board state, only a finite set of actions are valid. Can we precalculate all possible states, and compress those into a __policy__ for our chess playing agent? Using the policy, the agent will calculate for the current state, what is the best action, and will pick it for its next move. It is hard to design such a policy for chess, with all our understanding of the dynamics of the game, yet maybe with ML way, we can achieve such a policy.

Let's think about building such a policy. One way can be indeed, to devise a function that given a board state, returns the best action. Mathematically, we'll call such a function \pi<sup>*</sup>(s), or the optimal policy. An alternative can be a function, that given a board state, and an action returns a value that represents how good that move is. We can then quickly compare all valid moves and pick the one that leads to the highest value.  