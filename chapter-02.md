## Chapter 2. Ingridiant of a Reinforcement Learning setting

We already mentioned an __agent__ acting in an __environment__. What do we know about the environment? Sometimes we know a lot, as in the chess game. We know the rules. The agent, a software that plays one of the sides, should make the moves that will make it win or at least be a strong contestant. The unknowns are the moves of the other player, for example a human. We can guess what he/she will do, but there is no certainty about it. A __state__ in the chess game is natively the board state, when the turn is ours (of the agent). From that state (for example the starting position when we are playing the White), the agent will need to make a sequence of decisions regarding its moves, or __actions__. While the agent can plan in advance a few moves ahead, only the current move is taken and communicated to the environment (which in turn will inform the human player). When the next turn comes, we learn from the environment about the move that the human took. The original plan can be followed if still relevant, or another plan needs to be decided on. Given a board state, only a finite set of actions are valid. Can we precalculate all possible states, and compress those into a __policy__ for our chess playing agent? Using the policy, the agent will calculate for the current state, what is the best action, and will pick it for its next move. It is hard to design such a policy for chess, with all our understanding of the dynamics of the game, yet maybe with ML way, we can achieve such a policy. In many uses of RL, we even know less about the environment. In some settings we have a model of the environment or can learn such a model from various sources. Sometimes the only way to learn is to try ourselves on the go. Sometimes the environment is a moving target and the agent needs to continuesly adjust its beliefs.

Let's think about building such a policy. One way can be indeed, to devise a function that given a board state, returns the best action. Mathematically, we'll call such a function *&pi;<sup>\*</sup>(s)*, or an __optimal policy__. An alternative can be a function, that given a board state, and an action returns a value that represents how good that move is, *q<sup>\*</sup>(s, a)*, or the __optimal state-action function__, also refered as the __optimal Q__ function. We can then quickly compare all valid moves and pick the one that leads to the highest value. What exactly is this value that is returned by the *Q* function? With RL, we assume that all behavior can be translated into a __reward__ scheme. The goal of the agent is to maximize the total reward. By doing so the agent will achieve the desired behavior (of winning chess for example).
We seen before a reward of +1 for a win in the chess game, and a reward of -1 for a lose. If you consider a maze, where the desired policy is to find the way out, then a small negative reward, for example -0.1 for every time step can encourage an agent to find the quickest way out. If we want the agent to survive as long as possible, a small positive reward on every time step can help. Is this assumption that with a proper real-numbers reward scheme we can communicate any desired behavior true? Does it always hold? I'm not sure if a formal proof was provided, yet this belief worked in many use cases and till further discoveries is the way to go. So yes, by desiging a suitable reward scheme we can convay the desired behavior to a software agent. Easier said than done, but that is the spirit. So what is the relation of the reward function and the value returned by *Q*? A reward is given here and there around the way, for example a reward of +1 for "waking up" in a winning state, or -1, when it is a losing state. This reward given in a winning state, should somehow be reflected on moves that lead to that end game. For example, if given the current board state *s*, we can check-mate the opponent by making a specific move *a*, then the value for *q<sup>\*</sup>(s, a)* should be also +1. The implicit policy, will then pick the relevant move *a* and win. There is a third way to approach the challenge of building the policy. We can learn the __optimal state function__, or *V<sup>\*</sup>(s)*. Given a board *s'*, this function *V(s')* shall return the board`s value as seen from the agent's prespective. A state function can be used as follows. Given the current state *s* we will check all legitimate moves. Unless won by us, from each board state the opponent can make his/her best action. Assume we know to calculate what will happen next. Then we realize that by taking action *a* we will find ourselves next in state *s'*. Picking the action *a* that will result with the best *V(s')* is the smart move. Again the value of the state function is based on rewards. While the idea resembles heuristic and Min-Max search (aknowledging that the opponent is doing what best for him/her rather what best for us), the values originate all the way from the win/loss/stalemate end-games, rather than based on evaluation of the board. This is not completely accurate; in practice we will learn a mapping from states to values, or from state-actions to values (in a ML way). Those mapping will be based on the end-games but then the propagation to our learnt functions from state to a value (in the case of *V*), or from state-action to a value (in the case of *Q*) will be very similar in nature to heuristics (that was found by optimization rather than hand crafted).

### Markov Decision Process (MDP)

MDPs are the fomulation of RL and are worth keeping in mind. The idea is that indeed an agent is acting in an environment. The agent learn about the environment by observing states and rewards. The agent acts in the environment, and learns about the consequences from the observations in the next time step. Many computational tasks can be formulated as MDPs. A robot can observe with its sensors (ex. a digital camera, and/or a GPS) in what state it is. It can then act in the environment by any means it has. What will happen next? The robot can have a __model__ of the real world, and can estimate what is going to happen next. But only when it reevaluates the new images and GPS reading the robot knows what actually happened. More than that, an unexpected Marsian may enter its field of view (for example if the robot is on Mars). In the case of the chess game, the state can e indeed the board game, and after taking a move, we wait to see what happens next, what will be the move of the human. We'll learn it from the new state of the board. When we enter a new board state, we may regret previous actions, but that does not serve us. We ned to concentrate on the current state and what to do next. In this respect it does not help to remember what led to the current state. The action we're going to take next is indifferent to the history. This is where Markov assumption is taken. History will be used to learn a policy, that's for certain, but the mathematical formulation of the computation challenge will still assume from the current state, an agent should make its decisions regardless what happened before. The learned policy indeed will not use history directly. The mapping will be from current state to action. Note that the state space as we define it, may differenciate between "standing in front of the locked door", and "standing in front of the locked door with the matching key in your hand".

What exactly are the rewards in an MDP settings? If we want software to learn to play a video game, the reward can be the score. Otherwise it is not clear. We need somehow to convey to the agent what is the desired behavior. It is important that the agent has a way to appriciate higher rewards, given to it implicitly, or to translate the observations to rewards in addition to states. The agent will collect rewards. In the case of video games, the difference between the current score and the previous score is the reward attributed to taking previous action at previous state and ending in current state. What important at the end is the total score, but attributing each increase to a specific event, where applicable, is more beneficial for the learning process. The agent needs to learn sequential decision making process. While the history is not important, the other direction, collecting most rewards in the future, is what we try to achieve with RL. The agent needs to balance short term expected rewards with long term expected rewards. For example eating a cookie by Pacman and being captured by the ghost, or avoiding the ghost to later reach multiple cookies. On the other hand, as far as we go in planning, we have less certainty that we'll actually get there. Therefore, it makes sense to discount rewards in the future (weight them less than immediate rewards). Discounting also facilitates in the design of convergence algorithms as to avoid a potential unbounded total reward in a continues setting. A __discount factor__ &gamma; is often used, where a value of *0* for &gamma; means that we only look at the immediate rewards, a value of *1* means that we don't discount, and a value of *0.9* is an example for discounting. The next reward in the example is then valued as 0.9 times the value seen there, while rewards farther in the future, are weighted with 0.9<sup>2</sup> etc.      
